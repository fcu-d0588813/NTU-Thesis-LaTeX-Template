% !TeX root = ../main.tex

\chapter{相關研究}

\section{人才招募分析（Recruitment Analysis）}

Chou, Yi-Chi, and Han-Yen Yu. \cite{chou2020based} 提取履歷中的特徵，使用歸一化公式計算提取特徵的分數，這些分數分別對五個領域的工作中計算技能、經歷以及特質的分數，還有DISC的分析，並且使用TF-IDF的公式計算簡歷與職缺內容的相關度來推薦相關職缺給求職者。Maheshwary, Saket, and Hemant Misra. \cite{siamesematching} 在孿生網路（Siamese Network）用使用卷積神經網路，工作要求及履歷為輸入，計算輸入之間的相似度，但他們忽略了句子間的語義資訊。Qin, Chuan, \etal \cite{APJFNN} 提出一一個基於循環神經網路（Recurrent neural network，RNN）的方法為Basic Person-Job Fit Neural Network (BPJFNN)，用兩個雙個長短期記憶（Bi-directional Long Short-Term Memory，BiLSTM）得到工作經驗及工作要求的語義表示，接著將語義表示輸入到一層的類神經網路中，得到配對分數，有鑑於在履歷或工作要求中，不同的詞語在不同的位置有不同的重要性，Qin, Chuan, \etal \cite{APJFNN} 在BPJFNN上加入了注意力機制（Attention），根據不同詞、不同句子之間的重要性得到工作要求及履歷工作經驗的表示，以及一條工作需求與各條工作經驗的相關度表示。接著用這些表示來預測配對分數。Zhu, Chen, \etal \cite{PJFNN} 提出Person-Job Fit Neural Network (PJFNN) 能將求職者過往的工作經驗來配對職位需求，工作需求及履歷中的工作經驗利用卷積神經網路（Convolutional Neural Networks，CNN）得到投影到同一個空間的表示，接著計算在這個空間中的工作需求表示與工作經驗表示之間的距離，這個距離就是他們的相似度，來當作他們是否配對的依據，他們只使用履歷中一部分的資料作為預測，對於。由於求職是一個雙向的過程，雙方的意願是很重要的，所以 Le, Ran, \etal \cite{IPJF} 提出Interpretable Person-Job Fitting(IPJF)將雇主與求職者的意願加入模型中，首先預測招募者對求職者的意願以及求職者對此工作的意願，利用預測雙邊意願的過程中產生的隱藏特徵來預測配對的機率。Bian, Shuqing, \etal \cite{transferMathcing} 應用遷移式學習（Transformer Learning）中領域自適應（Domain Adaptation）的方法在工作配對的問題，用結構對應學習演算法（Structural Correspondence Learning，SCL）得到遷移過後的工作要求及履歷表示，在計算配對表示（match representation）的過程也是可以遷移的，最後經過多層感知器（Multilayer Perceptron，MLP）來預測最終的配對結果，此方法可以解決樣本不充分的工作領域的配對問題。Bian, Shuqing, \etal \cite{MultiVeiwMatching} 除了單純對文本做配對之外，還做了一個基於關係的配對模組（Relation-based Matching Component），某個工作要求與其他工作要求相似程度很高時，那麼已經與其他工作要求配對到的履歷應該也會與這個工作要求相似，在訓練配對時將純文本配對模型加上關係的配對模組進行預測工作與履歷的配對程度。

\section{文本挖掘（Text Mining）}

傳統的方法將文本表示為詞袋（Bag-of-words），統計每個詞出現的次數。詞頻逆文檔頻率（Term frequency–inverse document frequency，TF-IDF）是為了解決詞袋無法分辨常用詞以及不同詞語對文本的重要性問題，TF-IDF可以過濾掉常見的及無關緊要的詞語，賦予關鍵字比較高的權重，詞袋跟TF-IDF都忽略詞語的順序。詞嵌入（Word Embedding）將詞對應到向量中的維度，將句子中的字轉成向量表示，並且考慮了句子中詞的順序，當字詞過多向量會變得很龐大。Word2Vec考慮到上下文，語意相似的詞有較近的距離，但是只看周圍幾個詞，詞向量資訊量不足，Doc2Vecc考慮詞序後算出代表一語句段落的向量。以上提到的都是靜態詞向量，無法解決一字多義的問題。ELMO（Embeddings from Language Models）可以解決同義詞的問題，每個詞向量是雙向語言模型不同層的資訊，能夠捕捉詞義與上下文的資訊，但是這兩個方向的模型其實是分開訓練的，只是在最後做了個簡單相加，導致在單個方向看不到另一個方向的詞，有時候句子中的字同時依賴左右兩個方向的某些詞。基於變換器的雙向編碼器表示技術（Bidirectional Encoder Representations from Transformers，BERT）使用雙向變壓器（Transformer），使用遮罩的預測方式可以理解雙向上下文的能力，而非單個方向。

Liu, Bang, \etal \cite{CIGmatching} 提出長文本的配對方法，將文本轉換成圖Concept Interaction Graph (CIG)，使用關鍵字提取演算法提取關鍵字，每個關鍵字為一個節點—概念(Concept)，每個句子會附加到相關的概念中，先做局部的配對即對概念中的句子學習每個節點的配對向量（matching vector），然後使用卷積神經網路提取配對向量的特徵，最後使用多層感知器（Multilayer Perceptron，MLP）在此特徵預測上預測兩個長文本的是否相似。

